ta.sources = ta_source
ta.sinks = ta_sink ta_sink2
ta.channels = ta_channel ta_channel2

# Describe/configure the source
ta.sources.ta_source.type = avro
ta.sources.ta_source.bind = 0.0.0.0
ta.sources.ta_source.port = 9999
ta.sources.ta_source.fileHeader = true

#and selector to copy
ta.sources.ta_source.selector.type = replicating
ta.sources.ta_source.selector.optional = ta_channel ta_channel2
# Describe the sink,year-month-day,hour,minutes,use for hdfs

# you need to know the problem between hdfs.batchSize and transactionCapacity
ta.sinks.ta_sink.type = hdfs
ta.sinks.ta_sink.hdfs.path = hdfs://master:9000/program/%{key}/20%y%m%d/
ta.sinks.ta_sink.hdfs.filePrefix = job_test_
ta.sinks.ta_sink.hdfs.fileType = DataStream
#ta.sinks.ta_sink.hdfs.round = true
#ta.sinks.ta_sink.hdfs.roundValue = 2
#ta.sinks.ta_sink.hdfs.roundUnit = minute
ta.sinks.ta_sink.hdfs.writeFormat = Text
ta.sinks.ta_sink.hdfs.useLocalTimeStamp = true
ta.sinks.ta_sink.hdfs.minBlockReplicas=1
# every 30MB generate one file
ta.sinks.ta_sink.hdfs.rollCount = 0
ta.sinks.ta_sink.hdfs.rollSize = 31573682
ta.sinks.ta_sink.hdfs.rollInterval = 0
# after idle 100s it will generate new file
ta.sinks.ta_sink.hdfs.idleTimeout=100
ta.sinks.ta_sink.hdfs.batchSize = 100
#ta.sinks.ta_sink.hdfs.roolSize = 0

ta.sinks.ta_sink2.type = org.apache.flume.sink.kafka.KafkaSink
ta.sinks.ta_sink2.topic = test3
ta.sinks.ta_sink2.brokerList = master:9092
ta.sinks.ta_sink2.requiredAcks = 1
ta.sinks.ta_sink2.batchSize = 100
# Use a channel which buffers events in memory
ta.channels.ta_channel.type = memory
ta.channels.ta_channel.capacity = 1000
ta.channels.ta_channel.transactionCapacity = 100

ta.channels.ta_channel2.type = memory
ta.channels.ta_channel2.capacity = 1000
ta.channels.ta_channel2.transactionCapacity = 100

# Bind the source and sink to the channel
ta.sources.ta_source.channels = ta_channel ta_channel2
ta.sinks.ta_sink.channel = ta_channel
ta.sinks.ta_sink2.channel = ta_channel2
